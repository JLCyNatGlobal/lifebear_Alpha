{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the original CSV file\n",
        "file_path = '/content/3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv'\n",
        "\n",
        "try:\n",
        "    # Step 1: Load the data into a DataFrame\n",
        "    df = pd.read_csv(file_path, delimiter=';')  # Adjust delimiter if necessary\n",
        "    print(\"Original data loaded successfully.\")\n",
        "    print(\"First few rows before renaming and formatting:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Step 2: Rename the specified columns\n",
        "    df = df.rename(columns={\n",
        "        'id': 'id_num',\n",
        "        'login_id': 'user_name',\n",
        "        'mail_address': 'email_address',\n",
        "        'birthday_on': 'DOB',\n",
        "        'created_at': 'start_date'\n",
        "    })\n",
        "\n",
        "    # Step 3: Format 'start_date' to only have time (HH-MM-SS)\n",
        "    if 'start_date' in df.columns:\n",
        "        df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce').dt.strftime('%H-%M-%S')\n",
        "        df['start_date'] = df['start_date'].fillna('NaN')  # Fill unconvertible values with 'NaN'\n",
        "        print(\"Problematic 'start_date' values have been filled with 'NaN'.\")\n",
        "\n",
        "    # Step 4: Save the updated DataFrame to a new CSV file\n",
        "    output_file_path = '/content/step_1.csv'\n",
        "    if os.path.exists(output_file_path):\n",
        "        print(f\"Warning: {output_file_path} already exists and will be overwritten.\")\n",
        "\n",
        "    df.to_csv(output_file_path, index=False)\n",
        "    print(f\"Data successfully saved to {output_file_path}.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found at the specified path: {file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-79BD2QAvVi",
        "outputId": "4b30144c-b228-47ac-89b4-beb9419637f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-b52df5828772>:9: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, delimiter=';')  # Adjust delimiter if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data loaded successfully.\n",
            "First few rows before renaming and formatting:\n",
            "   id    login_id              mail_address                          password  \\\n",
            "0   1    sugimoto   sugimoto@lifebear.co.jp  f0bac04aa1b45cf443d722d6f71c0250   \n",
            "1   2         kou  nakanishi@lifebear.co.jp  48207c322ee5bb156ffec9f08c960aaa   \n",
            "2   3      yusuke     yuozawa1208@gmail.com  048261a8024ce51d379eb53cc51aaf33   \n",
            "3   4  entyan1106        endo1106@gmail.com  cd77a9dac26260a104facda5665eb3ab   \n",
            "4   5      kuriki          kuriki@wavy4.com  a026597c294cc48cd20ae361f10cbab1   \n",
            "\n",
            "            created_at          salt birthday_on  gender  \n",
            "0  2012-01-13 22:54:05  yGwBKynnsctI  1984-11-09     0.0  \n",
            "1  2012-01-14 12:48:31  aha6EuRYCDvU  1986-11-13     0.0  \n",
            "2  2012-01-17 15:33:22  PVS59dPWk9BH  1984-12-08     0.0  \n",
            "3  2012-01-17 15:37:02  vLZI6TVCJowN  1987-11-06     0.0  \n",
            "4  2012-01-17 18:52:32  swFznWWk79fg  1986-10-21     0.0  \n",
            "Problematic 'start_date' values have been filled with 'NaN'.\n",
            "Data successfully saved to /content/step_1.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Step 1: Load the cleaned data from 'step_1.csv'\n",
        "cleaned_file_path = '/content/step_1.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(cleaned_file_path)\n",
        "    print(\"Cleaned data loaded successfully.\")\n",
        "    print(\"First few rows of the cleaned data:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Step 2: Calculate the number of rows per chunk\n",
        "    total_rows = len(df)\n",
        "    num_chunks = 5\n",
        "    chunk_size = math.ceil(total_rows / num_chunks)\n",
        "\n",
        "    # Step 3: Split the DataFrame into 5 chunks\n",
        "    for i in range(num_chunks):\n",
        "        start_index = i * chunk_size\n",
        "        end_index = (i + 1) * chunk_size if (i + 1) * chunk_size < total_rows else total_rows\n",
        "\n",
        "        # Get the chunk\n",
        "        chunk = df.iloc[start_index:end_index]\n",
        "\n",
        "        # Step 4: Save the chunk to a new CSV file\n",
        "        chunk_file_path = f'/content/lifebear_dataset_chunk_{i + 1}.csv'\n",
        "        chunk.to_csv(chunk_file_path, index=False)\n",
        "        print(f\"Chunk {i + 1} saved to '{chunk_file_path}'\")\n",
        "\n",
        "    print(\"All chunks processed and saved successfully.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found at the specified path: {cleaned_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9O5HJmiE6Ih",
        "outputId": "d9c80b04-f285-4bd1-bef4-0c417d59eaf0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-5bf6377e3232>:7: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(cleaned_file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data loaded successfully.\n",
            "First few rows of the cleaned data:\n",
            "   id_num   user_name             email_address  \\\n",
            "0       1    sugimoto   sugimoto@lifebear.co.jp   \n",
            "1       2         kou  nakanishi@lifebear.co.jp   \n",
            "2       3      yusuke     yuozawa1208@gmail.com   \n",
            "3       4  entyan1106        endo1106@gmail.com   \n",
            "4       5      kuriki          kuriki@wavy4.com   \n",
            "\n",
            "                           password start_date          salt         DOB  \\\n",
            "0  f0bac04aa1b45cf443d722d6f71c0250   22-54-05  yGwBKynnsctI  1984-11-09   \n",
            "1  48207c322ee5bb156ffec9f08c960aaa   12-48-31  aha6EuRYCDvU  1986-11-13   \n",
            "2  048261a8024ce51d379eb53cc51aaf33   15-33-22  PVS59dPWk9BH  1984-12-08   \n",
            "3  cd77a9dac26260a104facda5665eb3ab   15-37-02  vLZI6TVCJowN  1987-11-06   \n",
            "4  a026597c294cc48cd20ae361f10cbab1   18-52-32  swFznWWk79fg  1986-10-21   \n",
            "\n",
            "   gender  \n",
            "0     0.0  \n",
            "1     0.0  \n",
            "2     0.0  \n",
            "3     0.0  \n",
            "4     0.0  \n",
            "Chunk 1 saved to '/content/lifebear_dataset_chunk_1.csv'\n",
            "Chunk 2 saved to '/content/lifebear_dataset_chunk_2.csv'\n",
            "Chunk 3 saved to '/content/lifebear_dataset_chunk_3.csv'\n",
            "Chunk 4 saved to '/content/lifebear_dataset_chunk_4.csv'\n",
            "Chunk 5 saved to '/content/lifebear_dataset_chunk_5.csv'\n",
            "All chunks processed and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the cleaned data from 'step_1.csv'\n",
        "cleaned_file_path = '/content/step_1.csv'\n",
        "\n",
        "try:\n",
        "    # Load the cleaned data\n",
        "    df = pd.read_csv(cleaned_file_path)\n",
        "    print(\"Cleaned data loaded successfully.\")\n",
        "    print(\"First few rows of the cleaned data:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Step 2: Calculate the number of rows per chunk\n",
        "    total_rows = len(df)\n",
        "    num_chunks = 5\n",
        "    chunk_size = math.ceil(total_rows / num_chunks)\n",
        "\n",
        "    # Initialize a DataFrame to collect discrepancies\n",
        "    garbage_df = pd.DataFrame()\n",
        "\n",
        "    # Step 3: Split the DataFrame into 5 chunks\n",
        "    for i in range(num_chunks):\n",
        "        start_index = i * chunk_size\n",
        "        end_index = (i + 1) * chunk_size if (i + 1) * chunk_size < total_rows else total_rows\n",
        "\n",
        "        # Get the chunk\n",
        "        chunk = df.iloc[start_index:end_index]\n",
        "\n",
        "        # Step 4: Identify rows with missing values or discrepancies in the chunk\n",
        "        # We will assume that any row with missing values is considered a discrepancy\n",
        "        discrepancies = chunk[chunk.isnull().any(axis=1)]\n",
        "\n",
        "        # If discrepancies are found, add them to the garbage DataFrame and remove from the current chunk\n",
        "        if not discrepancies.empty:\n",
        "            print(f\"Discrepancies found in chunk {i + 1}\")\n",
        "            garbage_df = pd.concat([garbage_df, discrepancies])\n",
        "            chunk = chunk.drop(discrepancies.index)\n",
        "\n",
        "        # Step 5: Save the cleaned chunk to a new CSV file\n",
        "        chunk_file_path = f'/content/lifebear_dataset_chunk_{i + 1}.csv'\n",
        "        chunk.to_csv(chunk_file_path, index=False)\n",
        "        print(f\"Chunk {i + 1} saved to '{chunk_file_path}'\")\n",
        "\n",
        "    # Step 6: Save the discrepancies to 'garbage.csv' if any were found\n",
        "    garbage_file_path = '/content/garbage.csv'\n",
        "    if not garbage_df.empty:\n",
        "        garbage_df.to_csv(garbage_file_path, index=False)\n",
        "        print(f\"Discrepancies saved to '{garbage_file_path}'\")\n",
        "    else:\n",
        "        print(\"No discrepancies found in any of the chunks.\")\n",
        "\n",
        "    print(\"All chunks processed and saved successfully.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"File not found at the specified path: {cleaned_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2mov97P_hYG",
        "outputId": "b8601223-d185-4e36-c4d7-7474ec03240d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-d146e5c13e05>:6: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(cleaned_file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data loaded successfully.\n",
            "First few rows of the cleaned data:\n",
            "   id_num   user_name             email_address  \\\n",
            "0       1    sugimoto   sugimoto@lifebear.co.jp   \n",
            "1       2         kou  nakanishi@lifebear.co.jp   \n",
            "2       3      yusuke     yuozawa1208@gmail.com   \n",
            "3       4  entyan1106        endo1106@gmail.com   \n",
            "4       5      kuriki          kuriki@wavy4.com   \n",
            "\n",
            "                           password start_date          salt         DOB  \\\n",
            "0  f0bac04aa1b45cf443d722d6f71c0250   22-54-05  yGwBKynnsctI  1984-11-09   \n",
            "1  48207c322ee5bb156ffec9f08c960aaa   12-48-31  aha6EuRYCDvU  1986-11-13   \n",
            "2  048261a8024ce51d379eb53cc51aaf33   15-33-22  PVS59dPWk9BH  1984-12-08   \n",
            "3  cd77a9dac26260a104facda5665eb3ab   15-37-02  vLZI6TVCJowN  1987-11-06   \n",
            "4  a026597c294cc48cd20ae361f10cbab1   18-52-32  swFznWWk79fg  1986-10-21   \n",
            "\n",
            "   gender  \n",
            "0     0.0  \n",
            "1     0.0  \n",
            "2     0.0  \n",
            "3     0.0  \n",
            "4     0.0  \n",
            "Discrepancies found in chunk 1\n",
            "Chunk 1 saved to '/content/lifebear_dataset_chunk_1.csv'\n",
            "Discrepancies found in chunk 2\n",
            "Chunk 2 saved to '/content/lifebear_dataset_chunk_2.csv'\n",
            "Discrepancies found in chunk 3\n",
            "Chunk 3 saved to '/content/lifebear_dataset_chunk_3.csv'\n",
            "Discrepancies found in chunk 4\n",
            "Chunk 4 saved to '/content/lifebear_dataset_chunk_4.csv'\n",
            "Discrepancies found in chunk 5\n",
            "Chunk 5 saved to '/content/lifebear_dataset_chunk_5.csv'\n",
            "Discrepancies saved to '/content/garbage.csv'\n",
            "All chunks processed and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of chunk file paths\n",
        "chunk_files = [\n",
        "    '/content/lifebear_dataset_chunk_1.csv',\n",
        "    '/content/lifebear_dataset_chunk_2.csv',\n",
        "    '/content/lifebear_dataset_chunk_3.csv',\n",
        "    '/content/lifebear_dataset_chunk_4.csv',\n",
        "    '/content/lifebear_dataset_chunk_5.csv'\n",
        "]\n",
        "\n",
        "try:\n",
        "    # Step 1: Initialize an empty list to collect DataFrames\n",
        "    dataframes = []\n",
        "\n",
        "    # Step 2: Load each chunk and append it to the list\n",
        "    for chunk_file in chunk_files:\n",
        "        if os.path.exists(chunk_file):\n",
        "            df = pd.read_csv(chunk_file)\n",
        "            dataframes.append(df)\n",
        "            print(f\"{chunk_file} loaded successfully.\")\n",
        "        else:\n",
        "            print(f\"Warning: {chunk_file} not found and will be skipped.\")\n",
        "\n",
        "    # Step 3: Filter out empty or all-NA DataFrames\n",
        "    filtered_dataframes = [\n",
        "        df for df in dataframes if not df.empty and not df.isnull().all().all()\n",
        "    ]\n",
        "\n",
        "    # Step 4: Concatenate all filtered DataFrames into a single DataFrame\n",
        "    if filtered_dataframes:\n",
        "        merged_df = pd.concat(filtered_dataframes, ignore_index=True)\n",
        "        print(\"All chunks merged successfully.\")\n",
        "\n",
        "        # Step 5: Save the merged DataFrame to 'clean_lifebear.csv'\n",
        "        output_file_path = '/content/clean_lifebear.csv'\n",
        "        merged_df.to_csv(output_file_path, index=False)\n",
        "        print(f\"Data successfully saved to {output_file_path}.\")\n",
        "    else:\n",
        "        print(\"No valid chunk files were loaded, so the merging process could not be completed.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP-dLIsmC9Vq",
        "outputId": "a46b4ad6-fdf1-46e4-bd67-b08ce5d95e55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lifebear_dataset_chunk_1.csv loaded successfully.\n",
            "/content/lifebear_dataset_chunk_2.csv loaded successfully.\n",
            "/content/lifebear_dataset_chunk_3.csv loaded successfully.\n",
            "/content/lifebear_dataset_chunk_4.csv loaded successfully.\n",
            "/content/lifebear_dataset_chunk_5.csv loaded successfully.\n",
            "All chunks merged successfully.\n",
            "Data successfully saved to /content/clean_lifebear.csv.\n"
          ]
        }
      ]
    }
  ]
}